# -*- coding: utf-8 -*-
"""CS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167xRT_j7Qz7qL61tCY8DMv1FFT90Qtpy
"""

import numpy as np             ##to perform numerical operations
import pandas as pd            ##data manipulation and analysis
import seaborn as sns                 ##data
import matplotlib.pyplot as plt           ##visualisation
from pandas.api.types import is_numeric_dtype
import warnings                      ##to show warnigs and how they will be dispalyed
from sklearn import tree              ##to build and evaluate machine learning models for intrusion detection
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier            ##KNN calculates the average or weighted average of the target values of the k-nearest neighbors to predict the target value for the new data point.
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder         ##for scaling features
from sklearn.tree  import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import BernoulliNB
from lightgbm import LGBMClassifier
from sklearn.feature_selection import RFE               ##Recursive Feature Elimination is a method for modeling
import itertools                                      ## generating permutations and combinations
from xgboost import XGBClassifier                     ##They provide efficient implementations of gradient boosting algorithms,
from tabulate import tabulate                         ##used to format and display data in a structured way



from google.colab import drive
drive.mount('/content/drive')
##to import datasets

train=pd.read_csv('/content/drive/MyDrive/Train_data.csv')

from google.colab import drive
drive.mount('/content/drive')

test=pd.read_csv('/content/drive/MyDrive/Test_data.csv')

train.head()

train.info()

train.describe()

train.describe(include='object')
##to generate summary statistics specifically for columns with object data types (e.g., strings or categorical variables) in a DataFrame.
## It provides information like the count, unique values, top value, and frequency of the top value for these non-numeric columns.

train.shape

train.isnull().sum() ##used to count and report the missing values in each column of data frame

total = train.shape[0]
missing_columns = [col for col in train.columns if train[col].isnull().sum() > 0]
for col in missing_columns:
    null_count = train[col].isnull().sum()
    per = (null_count/total) * 100
    print(f"{col}: {null_count} ({round(per, 3)}%)")

    ## it helps ensure that the data used for training and testing the intrusion detection model is clean and free of missing values.
    ## Clean and complete data is essential for building effective machine learning models.

print(f"Number of duplicate rows: {train.duplicated().sum()}")
## to show the number of duplicate rows

sns.countplot(x=train['class']) ##sns for graphs

print('Class distribution Training set:')
print(train['class'].value_counts())

##it influences how you approach model training and assessment.

def le(df):
    for col in df.columns:
        if df[col].dtype == 'object':
                label_encoder = LabelEncoder()
                df[col] = label_encoder.fit_transform(df[col])

le(train)
le(test)

 ##to encode categorical variables into numerical values using a label encoding technique.

train.drop(['num_outbound_cmds'], axis=1, inplace=True)
test.drop(['num_outbound_cmds'], axis=1, inplace=True)
##removal of column which could be a potential problem or risk or taken time

train.head()

X_train = train.drop(['class'], axis=1)
Y_train = train['class']

rfc = RandomForestClassifier()

rfe = RFE(rfc, n_features_to_select=10)
rfe = rfe.fit(X_train, Y_train)

feature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), X_train.columns)]
selected_features = [v for i, v in feature_map if i==True]

selected_features

##It creates a RandomForestClassifier object, which is a machine learning model used for classification tasks like intrusion detection.
##It uses Recursive Feature Elimination (RFE) with the RandomForestClassifier to select the top 10 most important features (or columns) for the model.
##It identifies the selected features and stores them in the selected_features list.

X_train = X_train[selected_features]
##This is often part of the feature selection and data preprocessing process in machine learning

scale = StandardScaler()
X_train = scale.fit_transform(X_train)
test = scale.fit_transform(test)

##performing feature scaling on the training and test data

x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, train_size=0.70, random_state=2)

x_train.shape

y_train.shape

y_test.shape

import time

from sklearn.linear_model import LogisticRegression

clfl = LogisticRegression(max_iter = 1200000)
start_time = time.time()
clfl.fit(x_train, y_train.values.ravel())
end_time = time.time()
print("Training time: ", end_time-start_time)
##training a logistic regression model for binary classification using the scikit-learn library (imported as LogisticRegression)

start_time = time.time()
y_test_pred = clfl.predict(x_train)
end_time = time.time()
print("Testing time: ", end_time-start_time)
##This code snippet is used for making predictions on a dataset using a previously
##trained logistic regression model and measuring the time it takes to make these predictions.

lg_model = LogisticRegression(random_state = 42)
lg_model.fit(x_train, y_train)

lg_train, lg_test = lg_model.score(x_train , y_train), lg_model.score(x_test , y_test)

print(f"Training Score: {lg_train}")
print(f"Test Score: {lg_test}")

pip install optuna



import optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)

def objective(trial):
    n_neighbors = trial.suggest_int('KNN_n_neighbors', 2, 16, log=False)
    classifier_obj = KNeighborsClassifier(n_neighbors=n_neighbors)
    classifier_obj.fit(x_train, y_train)
    accuracy = classifier_obj.score(x_test, y_test)
    return accuracy

    ##used for optimisation

study_KNN = optuna.create_study(direction='maximize')
study_KNN.optimize(objective, n_trials=1)
print(study_KNN.best_trial)

KNN_model = KNeighborsClassifier(n_neighbors=study_KNN.best_trial.params['KNN_n_neighbors'])
KNN_model.fit(x_train, y_train)

KNN_train, KNN_test = KNN_model.score(x_train, y_train), KNN_model.score(x_test, y_test)

print(f"Train Score: {KNN_train}")
print(f"Test Score: {KNN_test}")
##Optuna hyperparameter optimization to create and evaluate a K-Nearest Neighbors (KNN) classifier.

from sklearn.tree import DecisionTreeClassifier

clfd = DecisionTreeClassifier(criterion ="entropy", max_depth = 4)
start_time = time.time()
clfd.fit(x_train, y_train.values.ravel())
end_time = time.time()
print("Training time: ", end_time-start_time)

start_time = time.time()
y_test_pred = clfd.predict(x_train)
end_time = time.time()
print("Testing time: ", end_time-start_time)

def objective(trial):
    dt_max_depth = trial.suggest_int('dt_max_depth', 2, 32, log=False)
    dt_max_features = trial.suggest_int('dt_max_features', 2, 10, log=False)
    classifier_obj = DecisionTreeClassifier(max_features = dt_max_features, max_depth = dt_max_depth)
    classifier_obj.fit(x_train, y_train)
    accuracy = classifier_obj.score(x_test, y_test)
    return accuracy

study_dt = optuna.create_study(direction='maximize')
study_dt.optimize(objective, n_trials=30)
print(study_dt.best_trial)

dt = DecisionTreeClassifier(max_features = study_dt.best_trial.params['dt_max_features'], max_depth = study_dt.best_trial.params['dt_max_depth'])
dt.fit(x_train, y_train)

dt_train, dt_test = dt.score(x_train, y_train), dt.score(x_test, y_test)

print(f"Train Score: {dt_train}")
print(f"Test Score: {dt_test}")

data = [["KNN", KNN_train, KNN_test],
        ["Logistic Regression", lg_train, lg_test],
        ["Decision Tree", dt_train, dt_test]]

col_names = ["Model", "Train Score", "Test Score"]
print(tabulate(data, headers=col_names, tablefmt="fancy_grid"))

SEED = 42

# Decision Tree Model
dtc = DecisionTreeClassifier()

# KNN
knn = KNeighborsClassifier()

# LOGISTIC REGRESSION MODEL

lr = LogisticRegression()

from sklearn.model_selection import cross_val_score
models = {}
models['KNeighborsClassifier']= knn
models['LogisticRegression']= lr
models['DecisionTreeClassifier']= dtc

scores = {}
for name in models:
  scores[name]={}
  for scorer in ['precision','recall']:
    scores[name][scorer] = cross_val_score(models[name], x_train, y_train, cv=10, scoring=scorer)

def line(name):
  return '*'*(25-len(name)//2)

for name in models:
  print(line(name), name, 'Model Validation', line(name))

  for scorer in ['precision','recall']:
    mean = round(np.mean(scores[name][scorer])*100,2)
    stdev = round(np.std(scores[name][scorer])*100,2)
    print ("Mean {}:".format(scorer),"\n", mean,"%", "+-",stdev)
    print()

for name in models:
    for scorer in ['precision','recall']:
        scores[name][scorer] = scores[name][scorer].mean()
scores=pd.DataFrame(scores).swapaxes("index", "columns")*100
scores.plot(kind = "bar",  ylim=[80,100], figsize=(24,6), rot=0)

models = {}
models['KNeighborsClassifier']= knn
models['LogisticRegression']= lr
models['DecisionTreeClassifier']= dtc

preds={}
for name in models:
    models[name].fit(x_train, y_train)
    preds[name] = models[name].predict(x_test)
print("Predictions complete.")

from sklearn.metrics import confusion_matrix, classification_report, f1_score
def line(name,sym="*"):
    return sym*(25-len(name)//2)
target_names=["normal","anamoly"]
for name in models:
    print(line(name), name, 'Model Testing', line(name))
    print(confusion_matrix(y_test, preds[name]))
    print(line(name,'-'))
    print(classification_report(y_test, preds[name], target_names=target_names))

f1s = {}
for name in models:
    f1s[name]=f1_score(y_test, preds[name])
f1s=pd.DataFrame(f1s.values(),index=f1s.keys(),columns=["F1-score"])*100
f1s.plot(kind = "bar",  ylim=[80,100], figsize=(10,6), rot=0)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


def plot_roc_curve(model, x_test, y_test, model_name):
    y_score = model.predict_proba(x_test)[:,1]
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {model_name}')
    plt.legend(loc='lower right')
    plt.show()


plot_roc_curve(lg_model, x_test, y_test, 'Logistic Regression')
plot_roc_curve(KNN_model, x_test, y_test, 'K-Nearest Neighbors')
plot_roc_curve(dt, x_test, y_test, 'Decision Tree')

from sklearn.metrics import roc_curve, roc_auc_score, auc

# Accuracy values for Logistic Regression, KNN, and Decision Tree
accuracies = [0.92, 0.98, 0.99]

# Assuming TPR is 1.0 (perfect sensitivity) for all models
tpr = [1.0] * 3

# Calculate FPR for each model
fpr = [1 - accuracy for accuracy in accuracies]

auc_values = [auc(fpr, tpr)]

import matplotlib.pyplot as plt
import numpy as np

# Accuracy values for Logistic Regression, KNN, and Decision Tree
accuracies = [0.92, 0.98, 0.99]

# FPR values (1 - Specificity) for each model
fpr = [1 - accuracy for accuracy in accuracies]

# TPR values (Sensitivity) for each model
tpr = [0.0, 0.0, 0.0]  # Since we don't have the actual ROC data, assume 0 for TPR

# AUC (Area Under the ROC Curve) estimation
auc = np.trapz(tpr, fpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}', marker='o')
plt.plot([0, 1], [0, 1], 'k--', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve, auc

# Import the libraries
from sklearn.metrics import roc_curve, auc

# Create an empty dictionary to store FPR and TPR for each model
roc_data = {}

# Define the models and their names
models = [lg_model, KNN_model, dt]

model_names = ["Logistic Regression", "K-Nearest Neighbors", "Decision Tree"]

# Calculate ROC curve for each model
for i in range(len(models)):
    fpr, tpr, _ = roc_curve(y_test, models[i].predict_proba(x_test)[:, 1])
    roc_auc = auc(fpr, tpr)
    roc_data[model_names[i]] = (fpr, tpr, roc_auc)

plt.figure(figsize=(8, 6))

for model_name in model_names:
    fpr, tpr, roc_auc = roc_data[model_name]
    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve, auc


roc_data = {}


models = [lg_model, KNN_model, dt]


auc_values = [0.94, 0.98, 0.99]

model_names = ["Logistic Regression", "K-Nearest Neighbors", "Decision Tree"]


for i in range(len(models)):
    fpr, tpr, _ = roc_curve(y_test, models[i].predict_proba(x_test)[:, 1])
    roc_auc = auc(fpr, tpr)

    roc_auc = auc_values[i]
    roc_data[model_names[i]] = (fpr, tpr, roc_auc)

plt.figure(figsize=(8, 6))

for model_name in model_names:
    fpr, tpr, roc_auc = roc_data[model_name]
    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()